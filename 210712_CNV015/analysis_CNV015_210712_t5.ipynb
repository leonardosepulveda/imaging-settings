{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07/13/2021 | BC118: testing the effect of fixation \n",
    "\n",
    "The results of BC117 were inconclusive. I will run 4 experiments in the 1.1 section with full MERFISH runs to check the effect of different fixations. Those sections are small enough to allow a relatively fast test. Here I will use encoding probes in all samples. I will run a no-readout  control  at the beginning, along with DAPI, then perform the first hyb.  \n",
    "\n",
    "| sample\t| fixation |\n",
    "|---|---|\n",
    "|1\t| 4%PFA|\n",
    "|3\t| 95% EtOH|\n",
    "|5\t| 100% MeOH|\n",
    "|6\t| 100 Acetone|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env. path: C:\\Users\\Leonardo\\anaconda3\\envs\\scanpy_env_210121\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup environmental variables in windows\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "env_p = sys.prefix  # path to the env\n",
    "print(\"Env. path: {}\".format(env_p))\n",
    "new_p = ''\n",
    "for extra_p in (r\"Library\\mingw-w64\\bin\",\n",
    "    r\"Library\\usr\\bin\",\n",
    "    r\"Library\\bin\",\n",
    "    r\"Scripts\",\n",
    "    r\"bin\"):\n",
    "    new_p +=  os.path.join(env_p, extra_p) + ';'\n",
    "os.environ[\"PATH\"] = new_p + os.environ[\"PATH\"]  # set it for Python\n",
    "os.putenv(\"PATH\", os.environ[\"PATH\"])  # push it at the OS level\n",
    "\n",
    "# core libraries\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# optimizatin libraries\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# statistics libraries\n",
    "# import statistics as st\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# datatype libraries\n",
    "# from collections import Counter\n",
    "# from collections import defaultdict \n",
    "import pickle\n",
    "\n",
    "# plotting libraries\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "# from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "# from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "# image / segmentation libraries\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import cv2\n",
    "# from cellpose import utils\n",
    "from cellpose import models\n",
    "# from skimage import transform\n",
    "from skimage import morphology # erosion, dilation, opening, closing, white_tophat, black_tophat, skeletonize, convex_hull_image, disk\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# I/O libraries\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "# from sklearn import preprocessing \n",
    "# from sklearn.metrics import silhouette_samples\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sequence analysis libraries\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "# # single cell analysis libraries\n",
    "# import scanpy as sc\n",
    "# import scanpy.external as sce\n",
    "# sc.settings.verbosity = 2             # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "# sc.logging.print_header()\n",
    "\n",
    "# read my own libraries\n",
    "sys.path.insert(0,'C:/Software/python-functions/')\n",
    "import systemChecks as syschk # backupNotebook\n",
    "import imagedisplay as imdis \n",
    "import datareader as dr # readdax\n",
    "import miscAnalysis as ma \n",
    "\n",
    "# spot analysis libraries\n",
    "sys.path.insert(0,'C:/Software/')\n",
    "import ImageAnalysis3 as ia\n",
    "from ImageAnalysis3 import get_img_info, visual_tools, corrections, classes, alignment_tools, spot_tools\n",
    "\n",
    "# notebook appearenace\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expName = 'CNV015'\n",
    "expDate = '210712'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = 'c:/Users/Leonardo/Dropbox/research/analysis/CNVs/'\n",
    "remoteDir = f'x:/lsepulvedaduran/data/{expDate}_{expName}_data/'\n",
    "outputDir = f'g:/analysis/CNVs/{expName}/'\n",
    "sample = 1\n",
    "\n",
    "if not os.path.isdir(outputDir):\n",
    "    os.mkdir(outputDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Flat field correction.\n",
    "\n",
    "Ok, let's run the analysis for all fovs. I will use only hyb 0, and a single frame per stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = f'{outputDir}flat_field_correction/'\n",
    "if not os.path.isdir(outputFolder):\n",
    "    os.mkdir(outputFolder)\n",
    "\n",
    "for sample in [1]:#range(1,3,1):\n",
    "    outputFolder = f'{outputDir}flat_field_correction/sample_0{sample}/'\n",
    "    if not os.path.isdir(outputFolder):\n",
    "        os.mkdir(outputFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 240. Sample 1, Hyb  0, fov 161, channel 3. Elapsed time: 42.81 min,  11 sec per image,   0 min remaining\r"
     ]
    }
   ],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(250,250))\n",
    "filterSigma = 20\n",
    "filterSize = int(2*np.ceil(2*filterSigma)+1)+10\n",
    "t = time.time()\n",
    "\n",
    "channel_names = [750, 647, 488, 405]\n",
    "hyb = 0\n",
    "frames = list(range(0,140,4))\n",
    "\n",
    "nFOVs = 21*14\n",
    "nChannels = 4\n",
    "numImages = nChannels*nFOVs\n",
    "sample = 1\n",
    "\n",
    "counter = 0\n",
    "for fov in range(220,0,-1):\n",
    "    for channel in range(nChannels):    \n",
    "        fileName = f'{outputDir}flat_field_correction/sample_0{sample}/fcc_{channel}_{hyb:02d}_{fov:03d}.npy'\n",
    "\n",
    "        if not os.path.isfile(fileName):\n",
    "            imageStack = dr.DaxReader(f'{remoteDir}sample_0{sample}/hal_{fov:03d}.dax')\n",
    "\n",
    "            I = imageStack.loadAFrame(frames[15]+channel)\n",
    "\n",
    "            # tophat\n",
    "            WT = cv2.morphologyEx(I, cv2.MORPH_TOPHAT, kernel)\n",
    "\n",
    "            # Gaussian filter\n",
    "            G = cv2.GaussianBlur(I-WT,(filterSize,filterSize),filterSigma)\n",
    "\n",
    "            # save data\n",
    "            np.save(fileName,G)\n",
    "\n",
    "            # check the number of files in remote directories\n",
    "            existentFiles = []\n",
    "            for i in [1]:#range(1,3,1):\n",
    "                existentFiles.extend(os.listdir(f'{outputDir}flat_field_correction/sample_0{i}/'))\n",
    "\n",
    "            counter+=1\n",
    "\n",
    "            elapsed_time = (time.time() - t) \n",
    "            print(f'{counter:>4d}. Sample {sample}, Hyb{hyb:3d}, fov{fov:4d}, channel{channel:2d}. '\n",
    "                  f'Elapsed time: {elapsed_time/60:3.2f} min, '\n",
    "                  f'{elapsed_time/counter:3.0f} sec per image, '\n",
    "                  f'{(numImages-len(existentFiles))*elapsed_time/counter/60:3.0f} min remaining',end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyb = 0\n",
    "\n",
    "for sample in [1]:#range(1,3,1):\n",
    "    \n",
    "    if sample == 1:\n",
    "        microscope = 'mf3'    \n",
    "    else:\n",
    "        microscope = 'mf8' \n",
    "        \n",
    "    outputFile1 = f'ffc_raw_{microscope}.npy'\n",
    "    outputFile2 = f'ffc_num_{microscope}.npy'\n",
    "    outputFile3 = f'ffc_{microscope}.npy'\n",
    "    \n",
    "    if not (os.path.isfile(outputFile1) and os.path.isfile(outputFile2) and os.path.isfile(outputFile3)):\n",
    "\n",
    "        ffc = np.zeros((4,2048,2048))\n",
    "        ffc_n = np.zeros(4)\n",
    "        channel_name = [750, 647, 560, 405]\n",
    "        for channel in range(4):\n",
    "            for fov in range(130):\n",
    "\n",
    "                fileName = f'{outputDir}flat_field_correction/sample_0{sample}/fcc_{channel}_{hyb:02d}_{fov:03d}.npy'\n",
    "                \n",
    "                # only use images without saturated pixels\n",
    "                tmp = np.load(fileName)\n",
    "#                 print(np.amax(tmp))\n",
    "                if np.amax(tmp) < 10000:\n",
    "                    ffc[channel,:,:] += tmp\n",
    "                    ffc_n[channel] += 1\n",
    "                    print(f'Hyb{hyb:3d}, fov{fov:4d}, channel {channel_name[channel]:2d}.',end='\\r')\n",
    "                else:\n",
    "                    print(f'Hyb{hyb:3d}, fov{fov:4d}, channel {channel_name[channel]:2d} has high-intensity pixels. Skipping.')\n",
    "                    \n",
    "        np.save(outputFile1,ffc)  \n",
    "        np.save(outputFile2,ffc_n)\n",
    "        \n",
    "        f = np.zeros((4,2048,2048))\n",
    "        for channel in range(4):\n",
    "            f[channel,:,:] = ffc[channel,:,:].squeeze()/ffc_n[channel]\n",
    "            f[channel,:,:] = f[channel,:,:]/np.amax(f[channel,:,:])\n",
    "        \n",
    "        np.save(outputFile3,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the ffc look for each channel and previous exp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for sample in [1]:#range(1,3,1):\n",
    "    \n",
    "    if sample == 1:\n",
    "        fcc0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_mf3.npy')\n",
    "        fcc1 = np.load('ffc_mf3.npy')\n",
    "    else:\n",
    "        fcc0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_mf8.npy')\n",
    "        fcc1 = np.load('ffc_mf8.npy')\n",
    "\n",
    "    channel_name = [750, 650, 560, 405]\n",
    "    \n",
    "    imsize = 2\n",
    "    nrows = 3\n",
    "    ncols = 4\n",
    "    fig,ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(imsize*ncols,imsize*nrows))\n",
    "    \n",
    "    for ch in range(4):\n",
    "        \n",
    "        for i in range(3):\n",
    "            if i == 0:\n",
    "                f = fcc0[ch,:,:]\n",
    "            elif i == 1:\n",
    "                f = fcc1[ch,:,:]    \n",
    "            else:\n",
    "                f = np.abs(fcc1[ch,:,:]-fcc0[ch,:,:])\n",
    "        \n",
    "            ax[i, ch].imshow(f)\n",
    "            ax[i, ch].axis('off')\n",
    "            ax[i, ch].set_title(f'{channel_name[ch]} channel.\\nmin = {np.amin(f):1.3f}, max = {np.amax(f):1.3f}', fontsize=8)\n",
    "\n",
    "        \n",
    "    fig.tight_layout(pad=2,w_pad=0,h_pad=1)\n",
    "    if sample == 1:\n",
    "        fig.suptitle('MERFISH3')\n",
    "    else:\n",
    "        fig.suptitle('MERFISH8')\n",
    "    fig.savefig(f'j{expName}_07_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are similarities, Let's combine the data from previous experiments and see how things look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [1]:#range(1,3,1):\n",
    "    \n",
    "    if sample == 1:\n",
    "        ffc_raw_0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_raw_mf3.npy')\n",
    "        ffc_num_0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_num_mf3.npy')\n",
    "        ffc_raw_1 = np.load('ffc_raw_mf3.npy')\n",
    "        ffc_num_1 = np.load('ffc_num_mf3.npy')\n",
    "        outputFile = 'ffc_mf3_cb.npy'\n",
    "    elif sample == 2:\n",
    "        ffc_raw_0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_raw_mf8.npy')\n",
    "        ffc_num_0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_num_mf8.npy')\n",
    "        ffc_raw_1 = np.load('ffc_raw_mf8.npy')\n",
    "        ffc_num_1 = np.load('ffc_num_mf8.npy')\n",
    "        outputFile = 'ffc_mf8_cb.npy'\n",
    "    \n",
    "    f = np.zeros((4,2048,2048))\n",
    "    for channel in range(4):\n",
    "        ffc = ffc_raw_0[channel,:,:].squeeze() + ffc_raw_1[channel,:,:].squeeze()\n",
    "        ffc_n = ffc_num_0[channel] + ffc_num_1[channel]\n",
    "        f[channel,:,:] = ffc/ffc_n\n",
    "        f[channel,:,:] = f[channel,:,:]/np.amax(f[channel,:,:])\n",
    "    \n",
    "    np.save(outputFile,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different ffcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for sample in [1]:#range(1,3,1):\n",
    "    \n",
    "    if sample == 1:\n",
    "        fcc0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_mf3.npy')\n",
    "        fcc1 = np.load('ffc_mf3.npy')\n",
    "        fcc2 = np.load('ffc_mf3_cb.npy')\n",
    "    else:\n",
    "        fcc0 = np.load('c:/Users/Leonardo/Dropbox/research/analysis/CNVs/210516_CNV007_DNA_FISH/ffc_mf8.npy')\n",
    "        fcc1 = np.load('ffc_mf8.npy')\n",
    "        fcc2 = np.load('ffc_mf8_cb.npy')\n",
    "\n",
    "    channel_name = [750, 650, 560, 405]\n",
    "    \n",
    "    imsize = 2\n",
    "    nrows = 3\n",
    "    ncols = 4\n",
    "    fig,ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(imsize*ncols,imsize*nrows))\n",
    "    \n",
    "    for ch in range(4):\n",
    "        \n",
    "        for i in range(3):\n",
    "            if i == 0:\n",
    "                f = fcc0[ch,:,:]\n",
    "            elif i == 1:\n",
    "                f = fcc1[ch,:,:]    \n",
    "            else:\n",
    "                f = fcc2[ch,:,:] \n",
    "        \n",
    "            ax[i, ch].imshow(f)\n",
    "            ax[i, ch].axis('off')\n",
    "            ax[i, ch].set_title(f'{channel_name[ch]} channel.\\nmin = {np.amin(f):1.3f}, max = {np.amax(f):1.3f}', fontsize=8)\n",
    "\n",
    "        \n",
    "    fig.tight_layout(pad=2,w_pad=0,h_pad=1)\n",
    "    if sample == 1:\n",
    "        fig.suptitle('MERFISH3')\n",
    "    else:\n",
    "        fig.suptitle('MERFISH8')\n",
    "    fig.savefig(f'j{expName}_08_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for mf3 I will use the combined, for mf8 I will use the one from this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cellpose for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputFolder = f'{outputDir}cellpose/'\n",
    "if not os.path.isdir(outputFolder):\n",
    "    os.mkdir(outputFolder)\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "    outputFolder = f'{outputDir}cellpose/sample_0{sample}/'\n",
    "    if not os.path.isdir(outputFolder):\n",
    "        os.mkdir(outputFolder)\n",
    "\n",
    "    if sample == 1:\n",
    "        ffc = np.load('ffc_mf3_cb.npy')\n",
    "    else:\n",
    "        ffc = np.load('ffc_mf8.npy')\n",
    "\n",
    "    for i in range(130):\n",
    "        outputFile = f'{outputFolder}mask_{i:03d}.npy'\n",
    "        if not os.path.isfile(outputFile):\n",
    "            imageStack = dr.DaxReader( f'{remoteDir}sample_0{sample}/hal_{i:03d}.dax' )\n",
    "            I = imageStack.loadAFrame(15)\n",
    "            I = I / ffc[3,:,:].squeeze()\n",
    "            model_cyto = models.Cellpose(gpu=False, model_type='cyto')\n",
    "            masks_cyto, flows_cyto, styles_cyto, diams_cyto = model_cyto.eval(I, channels=[0,0],diameter=80)\n",
    "\n",
    "            np.save(outputFile, masks_cyto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cell contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if mosaics forlder exists\n",
    "if not os.path.isdir(f'{outputDir}cell_contours/'):\n",
    "    os.mkdir(f'{outputDir}cell_contours/')\n",
    "for sample in range(1,3,1):\n",
    "    if not os.path.isdir(f'{outputDir}cell_contours/sample_0{sample}/'):\n",
    "        os.mkdir(f'{outputDir}cell_contours/sample_0{sample}/')\n",
    "\n",
    "for sample in range(1,3,1): \n",
    "    for fov in range(130):\n",
    "        maskFilePath = f'{outputDir}cellpose/sample_0{sample}/mask_{fov:03d}.npy'\n",
    "        mask = np.load(maskFilePath)\n",
    "\n",
    "        cellIDs = np.unique(mask)\n",
    "        cellIDs = cellIDs[1:]\n",
    "    #     print(f'fov {fov}, {cellIDs.size} cells.')\n",
    "        for cell_id in range(cellIDs.size):\n",
    "\n",
    "            contourFileName = f'{outputDir}cell_contours/sample_0{sample}/contour_{fov}_{cellIDs[cell_id]}.npy'\n",
    "            if not os.path.isfile(contourFileName):\n",
    "\n",
    "                 # find cell coordinates \n",
    "                x,y = np.where(mask == cellIDs[cell_id])\n",
    "\n",
    "                # do not consider cells within 10 pixels from the border\n",
    "                if not (np.any(np.concatenate((x,y)) <10) or np.any(np.concatenate((x,y)) >2038)):\n",
    "\n",
    "                    # calculate cell contour\n",
    "                    contour, hierarchy  = cv2.findContours(np.uint8(mask == cellIDs[cell_id]), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)    \n",
    "                    np.save(contourFileName,contour[0])\n",
    "                    print(f'{contourFileName } created',end='\\r')\n",
    "            else:\n",
    "                print(f'{contourFileName } already exists, skipping.',end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets overlay the cell borders with the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get image indexes\n",
    "positions = pd.read_csv(f'C:/Software/imaging-settings/210526_CNV009/sample_01/positions_CNV009_10x13.txt', names=['X','Y'])\n",
    "indexes = positions.copy()\n",
    "indexes.X = ((positions.X-min(positions.X))/250).astype(int)\n",
    "indexes.Y = ((positions.Y-min(positions.Y))/250).astype(int)\n",
    "\n",
    "hyb_num = 0\n",
    "imsize = int(np.floor(2048/9))\n",
    "channel = 3\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "    outputFile = f'{outputDir}mosaics/sample_0{sample}/mosaic_0_{imsize}.npy'\n",
    "    \n",
    "    mosaic = np.zeros((imsize*10, imsize*13))\n",
    "    \n",
    "    # get list of cell contours\n",
    "    contoursFolder = f'{outputDir}cell_contours/sample_0{sample}/'\n",
    "    cellContourList = os.listdir(contoursFolder)\n",
    "\n",
    "    if  not os.path.isfile(outputFile):\n",
    "        for fov in range(130):\n",
    "            # get image\n",
    "            imName = f'hal_{fov:03d}'\n",
    "            selectedFrames = np.arange(0,140,4)+channel\n",
    "\n",
    "            imReader = dr.DaxReader(f'{remoteDir}sample_0{sample}/{imName}.dax')\n",
    "            image = imReader.loadAFrame(selectedFrames[15])\n",
    "            print(f'{imName}.dax read.', end='\\r')   \n",
    "            \n",
    "            # Get cell contours\n",
    "            fileList = [x for x in cellContourList if int(x.split('_')[1]) == fov]\n",
    "\n",
    "            # apply contours to image\n",
    "            mask = np.zeros((2048,2048))\n",
    "            for c in fileList:\n",
    "                contour = np.load(f'{contoursFolder}{c}')\n",
    "                contour = contour.squeeze()\n",
    "                mask[contour[:,1],contour[:,0]] = 1\n",
    "\n",
    "            mask = morphology.dilation(mask,morphology.selem.disk(5))\n",
    "            mask = np.array(mask, dtype='bool')\n",
    "            image[mask] = 2**16 - 1\n",
    "\n",
    "            im = Image.fromarray(image)\n",
    "            im = im.resize((imsize,imsize),resample=0)\n",
    "\n",
    "            mosaic[indexes.Y[fov]*imsize:(indexes.Y[fov]+1)*imsize, indexes.X[fov]*imsize:(indexes.X[fov]+1)*imsize] = im\n",
    "\n",
    "        np.save(outputFile, mosaic)  \n",
    "      \n",
    "    # plot mosaic overlay       \n",
    "    mosaic = np.load(f'{outputFile}')\n",
    "    xsize = 10\n",
    "    ysize = xsize*10/13\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(xsize, ysize))\n",
    "    vmax = [1000, 30000]\n",
    "    ax.imshow(mosaic, vmin=0, vmax=vmax[sample-1], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    for i in range(1,10,1):\n",
    "        ax.axhline(imsize*i, ls='--')\n",
    "    for i in range(1,13,1):\n",
    "        ax.axvline(imsize*i, ls='--')\n",
    "\n",
    "    txt = ax.text(mosaic.shape[0]/2, mosaic.shape[0]/10, f'{outputFile}', fontsize=10, va='center',ha='center',c='w')\n",
    "    txt.set_path_effects([PathEffects.withStroke(linewidth=2, foreground='k')])\n",
    "\n",
    "    fig.tight_layout(pad=0,w_pad=0,h_pad=0)\n",
    "    fig.savefig(f'j{expName}_09_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, the things look ok. The density I got is lower than Pu's experiments again. I need to Increase the density, or the number of fov. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting fitting of sample 1, fov 100, hyb 0, channel 0\n",
      "-- start fitting spots in channel:0, 92 seeded, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Software\\ImageAnalysis3\\External\\Fitting_v4.py:199: RuntimeWarning: overflow encountered in exp\n",
      "  return 2./(1+np.exp(t_))-1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 fitted in 65.901s.\n",
      "g:/analysis/CNVs/CNV015/spots/sample_01/hal_100_0.npy created after  94 seconds.\n",
      "saving g:/analysis/CNVs/CNV015/spots/sample_01/hal_100_0.npy...\n",
      "g:/analysis/CNVs/CNV015/spots/sample_01/hal_100_0.npy saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if mosaics forlder exists\n",
    "if not os.path.isdir(f'{outputDir}spots/'):\n",
    "    os.mkdir(f'{outputDir}spots/')\n",
    "for sample in [1]:\n",
    "    if not os.path.isdir(f'{outputDir}spots/sample_0{sample}/'):\n",
    "        os.mkdir(f'{outputDir}spots/sample_0{sample}/')\n",
    "\n",
    "\n",
    "ffc = np.load('ffc_mf3.npy')\n",
    "\n",
    "outputFolder = f'{outputDir}spots/'\n",
    "\n",
    "for fov in range(100,21*14,1):\n",
    "\n",
    "    for hyb in range(8):\n",
    "\n",
    "        # read the image file\n",
    "        if hyb == 0:\n",
    "            imName = f'hal_{fov:03d}'\n",
    "        else:\n",
    "            imName = f'hal_{fov:03d}_{hyb-1}'\n",
    "        imReader = dr.DaxReader(f'{remoteDir}sample_0{sample}/{imName}.dax')\n",
    "\n",
    "        # for each channel\n",
    "        for channel in range(2):\n",
    "            outputFile = f'{outputFolder}sample_0{sample}/{imName}_{channel}.npy'\n",
    "\n",
    "            # only run the analysis if file doest not exist already\n",
    "            if not os.path.exists(outputFile) and not os.path.exists(f'{outputFile}.tmp.npy'):\n",
    "                t = time.time()\n",
    "\n",
    "                # save empty array so other workers do not work in this array\n",
    "                np.save(f'{outputFile}.tmp.npy', np.array([])) \n",
    "\n",
    "                if hyb == 0:\n",
    "                    selectedFrames = np.arange(0,140,4)+channel\n",
    "                else:\n",
    "                    selectedFrames = np.arange(0,105,3)+channel\n",
    "\n",
    "                I = np.zeros((len(selectedFrames),2048,2048))\n",
    "\n",
    "                # get all images\n",
    "                for frame in range(len(selectedFrames)):\n",
    "                    I[frame,:,:] = imReader.loadAFrame(selectedFrames[frame])\n",
    "                    I[frame,:,:] = I[frame,:,:]/ffc[channel,:,:]\n",
    "\n",
    "                # try running the code\n",
    "                try:\n",
    "                    print(f'\\n\\nStarting fitting of sample {sample}, fov {fov}, hyb {hyb}, channel {channel}')\n",
    "                    spots = ia.spot_tools.fitting.fit_fov_image(I,channel, max_num_seeds=500, verbose=True)\n",
    "                    print(f'{outputFile} created after {time.time() -t:>3.0f} seconds.')\n",
    "                    print(f'saving {outputFile}...')\n",
    "                    np.save(outputFile, spots)  \n",
    "                    print(f'{outputFile} saved!\\n')\n",
    "\n",
    "                    # remove tmp file\n",
    "                    os.remove(f'{outputFile}.tmp.npy')\n",
    "                except:\n",
    "                    print(f'Something went wrong. {outputFile} was not created, Skipping.')\n",
    "            else:\n",
    "                print(f'{outputFile} already exists. Skipping.', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting takes ~1 min per fov channel.But I am ethernet limited, loading the data from Simon takes more than a minute. \n",
    "\n",
    "Ok, we have the cells and spots. First, let's compare the spot intensities for each hibrfidization run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sample in range(1,3,1):\n",
    "    \n",
    "    # read all the files in the folder\n",
    "    outputFolder = f'{outputDir}spots/sample_0{sample}/'\n",
    "\n",
    "    # create 2D list to keep the information\n",
    "    ncols, nrows = (8,2)\n",
    "    allSpots = [[0] * ncols for i in range(nrows)]\n",
    "\n",
    "    # get all the existent values\n",
    "    for hyb_num in range(8):\n",
    "        for channel_num in range(2):        \n",
    "            spotList = []\n",
    "            for fov in range(0,130,1): \n",
    "                # read the image file\n",
    "                if hyb_num == 0:\n",
    "                    imName = f'hal_{fov:03d}'\n",
    "                else:\n",
    "                    imName = f'hal_{fov:03d}_{hyb_num-1}'\n",
    "\n",
    "                fileName = f'{outputFolder}{imName}_{channel_num}.npy'\n",
    "\n",
    "                if os.path.exists(fileName):\n",
    "                    spots = np.load(fileName)\n",
    "                    # print(f'{fileName} shape = ({spots.shape}).')\n",
    "\n",
    "                    # calculate volume\n",
    "                    volume = ((math.pi)**(3/2))*spots[:,0]*np.sqrt(spots[:,5]*spots[:,6]*spots[:,7])\n",
    "                    volume = np.reshape(volume,(spots.shape[0],1))\n",
    "                    spots = np.concatenate((spots,volume), axis=1)\n",
    "\n",
    "                    # add channel to spotList\n",
    "                    spotList.append(spots)\n",
    "\n",
    "            combinedList =  np.concatenate(spotList)\n",
    "            allSpots[channel_num][hyb_num-1] = combinedList\n",
    "\n",
    "            print(f'Hyb: {hyb_num} Channel: {channel_num}, combinedList.shape = [{combinedList.shape[0]:7d}, {combinedList.shape[1]:3d}], {combinedList[0,0:3]}')\n",
    "\n",
    "    # Saving the objects:\n",
    "    with open(f'{outputDir}spots/sample_0{sample}/all_spots.pkl', 'wb') as f: \n",
    "        pickle.dump(allSpots, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelName = ['750', '647']\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "\n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/all_spots.pkl'\n",
    "    with open(outputFile,'rb') as f: \n",
    "        allSpots = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=12,figsize=(16,4))\n",
    "    xmin    = [2,  0,    0,    0, 2, 0.1, 0.1, 0.1, -1, -1, 1, 2.5]\n",
    "    xmax    = [5, 35, 2048, 2048, 5, 0.7, 0.7, 0.7,  1,  1, 4, 7]\n",
    "    xlabels = ['height','z','x','y','background','width_z','width_x','width_y','sin_theta','sin_phi','error','volume']\n",
    "    for channel in range(2):\n",
    "        for hyb in range(8):\n",
    "            for col in range(12):\n",
    "                values = allSpots[channel][hyb][:,col]\n",
    "\n",
    "                if (col>=1 and col<=3) or (col>=8 and col<=9):\n",
    "                    xlabel = xlabels[col]\n",
    "                else:\n",
    "                    values = np.log10(values+1)\n",
    "                    xlabel = f'log10({xlabels[col]})'\n",
    "\n",
    "                hist, bin_edges = np.histogram(values, bins=100)\n",
    "                bin_centers = bin_edges[0:-1] + (bin_edges[1] - bin_edges[0])/2   \n",
    "                ax[channel,col].plot(bin_centers,hist/max(hist),lw=1,alpha=0.5)\n",
    "                ax[channel,col].set_xlim([xmin[col], xmax[col]])\n",
    "\n",
    "                if col == 0:\n",
    "                    ax[channel,col].set_ylabel(f'{channelName[channel]} channel\\nspot counts')\n",
    "                if channel_num == 1:\n",
    "                    ax[channel,col].set_xlabel(xlabels[col])\n",
    "    \n",
    "#     fig.suptitle(f'sample {sample}')\n",
    "    fig.tight_layout(pad=3,w_pad=0,h_pad=0)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_10_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look a height and volume in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelName = ['750', '647']\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/all_spots.pkl'\n",
    "    with open(outputFile,'rb') as f: \n",
    "        allSpots = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=2,figsize=(10,4))\n",
    "\n",
    "    xlabels = ['height','volume']\n",
    "    for channel in range(2):\n",
    "        for hyb in range(8):\n",
    "            counter = 0\n",
    "            for col in [0,11]:\n",
    "\n",
    "                values = np.log10(allSpots[channel][hyb][:,col] + 1)\n",
    "                xlabel = f'log10({xlabels[counter]})'\n",
    "                ax[channel,counter].set_xlabel(xlabel)\n",
    "\n",
    "                hist, bin_edges = np.histogram(values, bins=200)\n",
    "                bin_centers = bin_edges[0:-1] + (bin_edges[1] - bin_edges[0])/2   \n",
    "                ax[channel,counter].plot(bin_centers,hist/max(hist),lw=1,alpha=0.5)\n",
    "                \n",
    "                if counter == 0:\n",
    "                    ax[channel,counter].set_xlim([2,5])\n",
    "                elif counter == 1:\n",
    "                    ax[channel,counter].set_xlim([3,7])\n",
    "\n",
    "                if col == 0:\n",
    "                    ax[channel,counter].set_ylabel(f'{channelName[channel]} channel\\nspot counts')\n",
    "                if ch == 2:\n",
    "                    ax[channel,counter].set_xlabel(xlabels[counter])\n",
    "\n",
    "                ax[channel,counter].set_yscale('log')\n",
    "                counter+=1\n",
    "\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=1,h_pad=1)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_11_{expDate}_{sample}_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, I was using the wrong variable for the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelName = ['750', '647']\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/all_spots.pkl'\n",
    "    with open(outputFile,'rb') as f: \n",
    "        allSpots = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=2,figsize=(10,5))\n",
    "\n",
    "    xlabels = ['height','volume']\n",
    "    for channel in range(2):\n",
    "        for hyb in range(8):\n",
    "            counter = 0\n",
    "            for col in [0,11]:\n",
    "\n",
    "                values = np.log10(allSpots[channel][hyb][:,col] + 1)\n",
    "                xlabel = f'log10({xlabels[counter]})'\n",
    "                ax[channel,counter].set_xlabel(xlabel)\n",
    "\n",
    "                hist, bin_edges = np.histogram(values, bins=200)\n",
    "                bin_centers = bin_edges[0:-1] + (bin_edges[1] - bin_edges[0])/2   \n",
    "                ax[channel,counter].plot(bin_centers,(10**hyb)*hist/max(hist),lw=1,alpha=0.5)\n",
    "                \n",
    "                if counter == 0:\n",
    "                    ax[channel,counter].set_xlim([2,5])\n",
    "                elif counter == 1:\n",
    "                    ax[channel,counter].set_xlim([3,7])\n",
    "\n",
    "                if col == 0:\n",
    "                    ax[channel,counter].set_ylabel(f'{channelName[channel]} channel\\nspot counts')\n",
    "                if ch == 2:\n",
    "                    ax[channel,counter].set_xlabel(xlabels[counter])\n",
    "\n",
    "                ax[channel,counter].set_yscale('log')\n",
    "                counter+=1\n",
    "\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=1,h_pad=1)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_11_{expDate}_{sample}_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no obvious increase of intensity of the spots with the hyb. or there is? The tendency is clearer looking at the height. Maybe if I calculate the mean height for spots over log10(H)=3.15 for the 750 channel and log10(H)=3.66 foir the 647 channel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of prbes per region\n",
    "selectedRegionsDF = pd.read_csv('selected_regions.csv')\n",
    "\n",
    "channelName = ['750', '647']\n",
    "channelColor = ['C3', 'C4']\n",
    "channelThreshold = [3.15, 3.66]\n",
    "ymax = [12000,22000]\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/all_spots.pkl'\n",
    "    with open(outputFile,'rb') as f: \n",
    "        allSpots = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=2,figsize=(10,5))\n",
    "    \n",
    "    meanHeight = np.zeros((8,2))\n",
    "    stdHeight = np.zeros((8,2))\n",
    "    for channel in range(2):\n",
    "        for hyb in range(8):\n",
    "\n",
    "            idx = np.log10(allSpots[channel][hyb][:,0] + 1) > channelThreshold[channel]\n",
    "            values_log = np.log10(allSpots[channel][hyb][idx,0] + 1)\n",
    "            values = allSpots[channel][hyb][idx,0]\n",
    "\n",
    "            hist, bin_edges = np.histogram(values_log, bins=10)\n",
    "            bin_centers = bin_edges[0:-1] + (bin_edges[1] - bin_edges[0])/2   \n",
    "            ax[channel,0].plot(bin_centers,hist/max(hist),lw=1.5,alpha=0.5)\n",
    "            ax[channel,0].set_xlabel('log10(Spot height)')\n",
    "            ax[channel,0].set_ylabel(f'{channelName[channel]} channel\\nspot counts\\nabove threshold')\n",
    "            \n",
    "            meanHeight[hyb,channel] = np.mean(values)\n",
    "            stdHeight[hyb,channel]  = np.std(values)\n",
    "\n",
    "        ax[channel,1].plot(selectedRegionsDF.a, meanHeight[:,channel],'o-',lw=1.5,alpha=0.5, c=channelColor[channel])\n",
    "        ax[channel,1].set_xlabel('Probes per region')\n",
    "        ax[channel,1].set_ylabel('Spot height (A.U.)')\n",
    "        ax[channel,1].axis([50,400,0,ymax[channel]])\n",
    "              \n",
    "    \n",
    "#                 hist, bin_edges = np.histogram(values, bins=200)\n",
    "#                 bin_centers = bin_edges[0:-1] + (bin_edges[1] - bin_edges[0])/2   \n",
    "#                 ax[channel,counter].plot(bin_centers,(10**hyb)*hist/max(hist),lw=1,alpha=0.5)\n",
    "                \n",
    "#                 if counter == 0:\n",
    "#                     ax[channel,counter].set_xlim([2,5])\n",
    "#                 elif counter == 1:\n",
    "#                     ax[channel,counter].set_xlim([3,7])\n",
    "\n",
    "#                 if col == 0:\n",
    "#                     ax[channel,counter].set_ylabel(f'{channelName[channel]} channel\\nspot counts')\n",
    "#                 if ch == 2:\n",
    "#                     ax[channel,counter].set_xlabel(xlabels[counter])\n",
    "\n",
    "#                 ax[channel,counter].set_yscale('log')\n",
    "#                 counter+=1\n",
    "\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=1,h_pad=1)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_12_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's check the 2D dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/all_spots.pkl'\n",
    "    with open(outputFile,'rb') as f: \n",
    "        allSpots = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=8,figsize=(16,4))\n",
    "\n",
    "    channels = [750,650]\n",
    "\n",
    "    for channel_num in range(2):\n",
    "        for hyb_num in range(8):\n",
    "            height = allSpots[channel_num][hyb_num][:,0]\n",
    "            volume = allSpots[channel_num][hyb_num][:,5]*allSpots[channel_num][hyb_num][:,6]*allSpots[channel_num][hyb_num][:,7]\n",
    "            ax[channel_num ,hyb_num].hexbin(height,volume,gridsize=50,xscale='log',yscale='log',extent=(2,5,-1,2),cmap=plt.cm.Blues,bins='log')\n",
    "            if channel_num == 1:\n",
    "                ax[channel_num ,hyb_num].set_xlabel('height')\n",
    "            if hyb_num == 0:\n",
    "                ax[channel_num ,hyb_num].set_ylabel(f'{channels[channel_num]} channel\\nSpot volume\\nw1*w2*w3')\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num ,hyb_num].set_title(f'{selectedRegionsDF.a.iloc[hyb_num].astype(\"int\")} probes', fontsize=10)\n",
    "\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num, hyb_num].set_xticklabels([])\n",
    "                ax[channel_num, hyb_num].set_xticks([])\n",
    "            if hyb_num > 0:   \n",
    "                ax[channel_num, hyb_num].set_yticklabels([])\n",
    "                ax[channel_num, hyb_num].set_yticks([])\n",
    "\n",
    "            ax[channel_num ,hyb_num].axis([1e2,1e5,1e-1,1e2])\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=0,h_pad=0)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_13_{expDate}_{sample}_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [750,650]\n",
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/all_spots.pkl'\n",
    "    with open(outputFile,'rb') as f: \n",
    "        allSpots = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "\n",
    "    for channel_num in range(2):\n",
    "        height = []\n",
    "        volume = []\n",
    "        for hyb_num in range(8):\n",
    "            height.extend(list(allSpots[channel_num][hyb_num][:,0]))\n",
    "            volume.extend(list(allSpots[channel_num][hyb_num][:,5]*allSpots[channel_num][hyb_num][:,6]*allSpots[channel_num][hyb_num][:,7]))\n",
    "\n",
    "        ax[channel_num].hexbin(height,volume,gridsize=50,xscale='log',yscale='log',extent=(2,5,-1,2),cmap=plt.cm.Blues,bins='log')\n",
    "        ax[channel_num].set_xlabel('height')\n",
    "        ax[channel_num].set_ylabel(f'{channels[channel_num]} channel\\nSpot volume\\nw1*w2*w3')\n",
    "        ax[channel_num].set_title(f'{selectedRegionsDF.a.iloc[hyb_num].astype(\"int\")} probes', fontsize=10)\n",
    "        ax[channel_num].axis([1e2,1e5,1e-1,1e2])\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=0,h_pad=0)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_13_{expDate}_{sample}_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation is clearer than before, similar to what I saw in Pu's data. But there I had way more data. \n",
    "\n",
    "# Allocate spots to inside or outside cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all cellpose masks into an array.\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "\n",
    "    cellposeFolder = f'{outputDir}cellpose/sample_0{sample}/'\n",
    "\n",
    "    if os.path.isfile(f'{cellposeFolder}bg_masks.npy'):\n",
    "        bg_masks = np.load(f'{cellposeFolder}bg_masks.npy')\n",
    "        cell_masks = np.load(f'{cellposeFolder}cell_masks.npy')\n",
    "    else:\n",
    "\n",
    "        fileList = sorted([x for x in os.listdir(f'{cellposeFolder}/') if 'npy' in x])\n",
    "\n",
    "        bg_masks = np.zeros((len(fileList),2048,2048))\n",
    "        cell_masks = np.zeros((len(fileList),2048,2048))\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(50,50))\n",
    "\n",
    "        for i in range(len(fileList)):\n",
    "            maskFilePath = f'{cellposeFolder}{fileList[i]}'\n",
    "\n",
    "            mask = np.load(maskFilePath)\n",
    "            mask = mask>0\n",
    "            bgMask = cv2.dilate(np.float32(mask),kernel,iterations = 1)\n",
    "            #bgMask = morphology.dilation(mask,morphology.selem.disk(50))\n",
    "            bgMask = bgMask<1\n",
    "\n",
    "            cell_masks[i,:,:] = mask.astype(int)\n",
    "            bg_masks[i,:,:] = bgMask.astype(int)\n",
    "\n",
    "            print(maskFilePath,end='\\r')\n",
    "\n",
    "        np.save(f'{cellposeFolder}bg_masks.npy',bg_masks)\n",
    "        np.save(f'{cellposeFolder}cell_masks.npy',cell_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(1,3,1): \n",
    "\n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/combined_spot_list.pkl'\n",
    "    if Path(outputFile).is_file():\n",
    "        print(f'opening {outputFile} ...')\n",
    "        with open(outputFile,'rb') as f: \n",
    "            combinedSpotList = pickle.load(f)\n",
    "    else:  \n",
    "        spotsFolder = f'{outputDir}spots/sample_0{sample}/'\n",
    "        cellposeFolder = f'{outputDir}cellpose/sample_0{sample}/'\n",
    "        bg_masks = np.load(f'{cellposeFolder}bg_masks.npy')\n",
    "        cell_masks = np.load(f'{cellposeFolder}cell_masks.npy')\n",
    "\n",
    "        # create list of lists for the data\n",
    "        ncols, nrows = (8,2)\n",
    "        combinedSpotList = [[0] * ncols for _ in range(nrows)]\n",
    "\n",
    "        for channel_num in range(2):\n",
    "            for hyb_num in range(8):\n",
    "\n",
    "                # get dummy list to get all fovs \n",
    "                spotList = []\n",
    "                for fov in range(130):\n",
    "                    \n",
    "                    print(f' Sample {sample}, channel {channel_num}, hyb {hyb_num}, fov {fov:>3d}', end='\\r')\n",
    "\n",
    "                    # getting cell masks\n",
    "                    mask = cell_masks[fov,:,:].squeeze()\n",
    "                    mask_bg = bg_masks[fov,:,:].squeeze()\n",
    "\n",
    "                    # reading spot list for specific fov/hyb/channel \n",
    "                    if hyb_num == 0:\n",
    "                        imName = f'hal_{fov:03d}'\n",
    "                    else:\n",
    "                        imName = f'hal_{fov:03d}_{hyb_num-1}'\n",
    "\n",
    "                    fileName = f'{spotsFolder}{imName}_{channel_num}.npy'\n",
    "\n",
    "                    # check if file exists\n",
    "                    if os.path.exists(fileName):\n",
    "                        spots = np.load(f'{fileName}')\n",
    "                        spot_class = np.zeros((spots.shape[0],1))  \n",
    "                        spot_int = np.zeros((spots.shape[0],1))\n",
    "                        spot_vol = np.zeros((spots.shape[0],1))\n",
    "\n",
    "                        for spot_num in range(spots.shape[0]):\n",
    "                            x = spots[spot_num,3]\n",
    "                            y = spots[spot_num,2]\n",
    "                            spot_int[spot_num] = ((math.pi)**(3/2))*spots[spot_num,0]*np.sqrt(spots[spot_num,5]*spots[spot_num,6]*spots[spot_num,7])\n",
    "                            spot_vol[spot_num] = spots[spot_num,5]*spots[spot_num,6]*spots[spot_num,7]\n",
    "\n",
    "                            if np.any(mask[np.floor(y).astype(int):np.ceil(y).astype(int), np.floor(x).astype(int):np.ceil(x).astype(int)]):\n",
    "                                spot_class[spot_num] = 1\n",
    "                            if np.any(mask_bg[np.floor(y).astype(int):np.ceil(y).astype(int), np.floor(x).astype(int):np.ceil(x).astype(int)]):\n",
    "                                spot_class[spot_num] = 2\n",
    "\n",
    "                        spots_cat = np.concatenate((spots[:,[0,3,2]], spot_int, spot_vol ,spot_class),axis=1)            \n",
    "                        spotList.append(spots_cat)\n",
    "\n",
    "                # combine all arrays\n",
    "                combinedSpotList[channel_num][hyb_num] = np.concatenate(spotList)\n",
    "\n",
    "        # Saving the objects:\n",
    "        with open(outputFile, 'wb') as f: \n",
    "            pickle.dump(combinedSpotList, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedRegionsDF = pd.read_csv('selected_regions.csv')\n",
    "\n",
    "for sample in range(1,3,1): \n",
    "\n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/combined_spot_list.pkl'\n",
    "\n",
    "    with open(outputFile,'rb') as f: \n",
    "        combinedSpotList = pickle.load(f)\n",
    "\n",
    "    n_spots = np.zeros((2,8))\n",
    "    for i in range(2):\n",
    "        for j in range(8):\n",
    "            n_spots[i,j] = combinedSpotList[i][j].shape[0]\n",
    "\n",
    "    fig,ax = plt.subplots(ncols=1, nrows=1, figsize=(10,5))\n",
    "    ax.plot(selectedRegionsDF.a, n_spots[0,:],'o-',c='C0',label='750')\n",
    "    ax.plot(selectedRegionsDF.a, n_spots[1,:],'o-',c='C1',label='647')\n",
    "    ax.axis([0,400,0,100000])\n",
    "    ax.set_xlabel('Number of probes')\n",
    "    ax.set_ylabel('Number of spots')\n",
    "    ax.legend()\n",
    "\n",
    "    fig.savefig(f'j{expName}_14_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/combined_spot_list.pkl'\n",
    "\n",
    "    with open(outputFile,'rb') as f: \n",
    "        combinedSpotList = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=8,figsize=(16,4))\n",
    "\n",
    "    nbins = 50\n",
    "    for channel_num in range(2):\n",
    "        for hyb_num in range(8):\n",
    "\n",
    "            idx_cell = combinedSpotList[channel_num][hyb_num][:,5] == 1\n",
    "            idx_bg = combinedSpotList[channel_num][hyb_num][:,5] == 2\n",
    "            height = np.log10(combinedSpotList[channel_num][hyb_num][:,0])\n",
    "            spot_int = np.log10(combinedSpotList[channel_num][hyb_num][:,3])\n",
    "\n",
    "            xmin = 2 \n",
    "            xmax = 5\n",
    "\n",
    "            ax[channel_num, hyb_num].hist(height[idx_cell], bins=nbins, range=(xmin,xmax),color='C0',alpha=0.3,density=True, log=True)\n",
    "            ax[channel_num, hyb_num].hist(height[idx_bg],   bins=nbins, range=(xmin,xmax),color='C2',alpha=0.3,density=True, log=True)\n",
    "\n",
    "            ax[channel_num, hyb_num].text(0.6, 0.8,f'inside',c='C0',transform=ax[channel_num, hyb_num].transAxes) # cells\\n({sum(idx_cell)} spots)\n",
    "            ax[channel_num, hyb_num].text(0.6, 0.65,f'outside',c='C2',transform=ax[channel_num, hyb_num].transAxes) # cells\\n({sum(idx_bg)} spots)\n",
    "\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num ,hyb_num].set_title(f'{selectedRegionsDF.a.iloc[hyb_num].astype(\"int\")} probes', fontsize=10)\n",
    "            ax[channel_num, hyb_num].axis([xmin,xmax,0,3])\n",
    "\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num, hyb_num].set_xticklabels([])\n",
    "                ax[channel_num, hyb_num].set_xticks([])\n",
    "            if hyb_num > 0:   \n",
    "                ax[channel_num, hyb_num].set_yticklabels([])\n",
    "                ax[channel_num, hyb_num].set_yticks([])\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num, hyb_num].axvline(3.1,ls='--', alpha=0.5)\n",
    "            if channel_num == 1:\n",
    "                ax[channel_num, hyb_num].axvline(3.4,ls='--', alpha=0.5)\n",
    "\n",
    "    fig.add_subplot(111, frame_on=False)\n",
    "    plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "    plt.xlabel(\"log10(Density)\")\n",
    "    plt.ylabel(\"Spot height\")\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=0,h_pad=0)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_15_{expDate}_{sample}_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(1,3,1):\n",
    "    \n",
    "    outputFile = f'{outputDir}spots/sample_0{sample}/combined_spot_list.pkl'\n",
    "\n",
    "    with open(outputFile,'rb') as f: \n",
    "        combinedSpotList = pickle.load(f)\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=8,figsize=(16,4))\n",
    "\n",
    "    nbins = 50\n",
    "    for channel_num in range(2):\n",
    "        for hyb_num in range(8):\n",
    "\n",
    "            idx_cell = combinedSpotList[channel_num][hyb_num][:,5] == 1\n",
    "            idx_bg = combinedSpotList[channel_num][hyb_num][:,5] == 2\n",
    "            height = np.log10(combinedSpotList[channel_num][hyb_num][:,0])\n",
    "            spot_int = np.log10(combinedSpotList[channel_num][hyb_num][:,3])\n",
    "\n",
    "            xmin = 2 \n",
    "            xmax = 5\n",
    "\n",
    "            ax[channel_num, hyb_num].hist(height[idx_cell], bins=nbins, range=(xmin,xmax),color='C0',alpha=0.3,density=True, log=False)\n",
    "            ax[channel_num, hyb_num].hist(height[idx_bg],   bins=nbins, range=(xmin,xmax),color='C2',alpha=0.3,density=True, log=False)\n",
    "\n",
    "            ax[channel_num, hyb_num].text(0.6, 0.8,f'inside',c='C0',transform=ax[channel_num, hyb_num].transAxes) # cells\\n({sum(idx_cell)} spots)\n",
    "            ax[channel_num, hyb_num].text(0.6, 0.65,f'outside',c='C2',transform=ax[channel_num, hyb_num].transAxes) # cells\\n({sum(idx_bg)} spots)\n",
    "\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num ,hyb_num].set_title(f'{selectedRegionsDF.a.iloc[hyb_num].astype(\"int\")} probes', fontsize=10)\n",
    "            ax[channel_num, hyb_num].axis([xmin,xmax,0,3])\n",
    "\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num, hyb_num].set_xticklabels([])\n",
    "                ax[channel_num, hyb_num].set_xticks([])\n",
    "            if hyb_num > 0:   \n",
    "                ax[channel_num, hyb_num].set_yticklabels([])\n",
    "                ax[channel_num, hyb_num].set_yticks([])\n",
    "            if channel_num == 0:\n",
    "                ax[channel_num, hyb_num].axvline(3.1,ls='--', alpha=0.5)\n",
    "            if channel_num == 1:\n",
    "                ax[channel_num, hyb_num].axvline(3.4,ls='--', alpha=0.5)\n",
    "\n",
    "    fig.add_subplot(111, frame_on=False)\n",
    "    plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "    plt.xlabel(\"log10(Density)\")\n",
    "    plt.ylabel(\"Spot height\")\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=0,h_pad=0)  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    fig.savefig(f'j{expName}_15_{expDate}_{sample}_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, using x = 10 ** 3.1 and x = 10 ** 3.4 as thresholds for 750 and 650 respectively\n",
    "\n",
    "# Calculate DAPI intensity inside the cells. \n",
    "\n",
    "I realize that I made a mistake in the code below, I was asigning the wrong index to each cell, so the counts vs dapi plots gave me a flat line (the average, expected from random asignment)\n",
    "\n",
    "To run the anlysis again takes forever, because I need to read the images in the remote folder that has 1G connection. Let's copy the dapi images to the local folder, so if I need to do analysis again, I do not need to read remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "# check if mosaics forlder exists\n",
    "if not os.path.isdir(f'{outputDir}dapi_stacks/'):\n",
    "    os.mkdir(f'{outputDir}dapi_stacks/')\n",
    "\n",
    "for sample in range(1,3,1): \n",
    "\n",
    "    if not os.path.isdir(f'{outputDir}dapi_stacks/sample_0{sample}/'):\n",
    "        os.mkdir(f'{outputDir}dapi_stacks/sample_0{sample}/')\n",
    "\n",
    "    # get the flat field correction\n",
    "    if sample == 1:\n",
    "        ffc = np.load('ffc_mf3_cb.npy')\n",
    "    elif sample == 2:\n",
    "        ffc = np.load('ffc_mf8.npy')\n",
    "\n",
    "    for fov in range(130):\n",
    "\n",
    "        dapiFileName = f'{outputDir}dapi_stacks/sample_0{sample}/dapi_stack_{fov:03d}.npy'\n",
    "\n",
    "        if not os.path.isfile(dapiFileName):\n",
    "\n",
    "            imageStack = dr.DaxReader( f'{remoteDir}sample_0{sample}/hal_{fov:03d}.dax' )\n",
    "\n",
    "            # This step is very time variable. maybe because reading from the remote location \n",
    "            # depends on the address of the data on the disk and the usage of the connection \n",
    "            # at that particular period in time\n",
    "            frames = np.arange(3,140,4)\n",
    "            dapiStack = np.zeros((frames.size,2048,2048))\n",
    "            for i in range(frames.size):\n",
    "                I = imageStack.loadAFrame(frames[i]) \n",
    "                dapiStack[i,:,:] = I / ffc[3,:,:].squeeze() \n",
    "\n",
    "            np.save(dapiFileName, dapiStack)\n",
    "            print(f'Saving {dapiFileName}. Elapsed time {(time.time()-t)/60:>3.1f} min.',end='\\r')\n",
    "        else:\n",
    "            print(f'fov {fov:>3d} already processed. Skipping.', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate the dapi intensity within each nucleus for each z position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "# check if mosaics forlder exists\n",
    "if not os.path.isdir(f'{outputDir}dapi_int/'):\n",
    "    os.mkdir(f'{outputDir}dapi_int/')\n",
    "\n",
    "for sample in range(1,3,1): \n",
    "\n",
    "    if not os.path.isdir(f'{outputDir}dapi_int/sample_0{sample}/'):\n",
    "        os.mkdir(f'{outputDir}dapi_int/sample_0{sample}/')\n",
    "\n",
    "    for fov in range(130):\n",
    "        \n",
    "        # Get cell masks\n",
    "        cellposeFolder = f'{outputDir}cellpose/sample_0{sample}/'\n",
    "        maskFilePath = f'{cellposeFolder}/mask_{fov:03d}.npy'\n",
    "        mask = np.load(maskFilePath)\n",
    "\n",
    "        outputFileName = f'{outputDir}dapi_int/sample_0{sample}/dapi_int_{fov:03d}.npy'\n",
    "\n",
    "        if not os.path.isfile(outputFileName):\n",
    "\n",
    "            # Read local copy of dapi stack\n",
    "            dapiStack = np.load(f'{outputDir}dapi_stacks/sample_0{sample}/dapi_stack_{fov:03d}.npy')\n",
    "            \n",
    "            cellIDs = np.unique(mask)\n",
    "            cellIDs = cellIDs[1:] # Discard 0 (background)\n",
    "            dapiIntensity = np.zeros(( max(cellIDs)+1,frames.size)) # the row index is the cell index in the mask\n",
    "\n",
    "            for cell_id in range(cellIDs.size):\n",
    "\n",
    "                print(f'fov = {fov:>3d}, cell = {cell_id:>3d}, Elapsed time: {(time.time()-t)/60:>3.2f} min.',end='\\r')\n",
    "\n",
    "                 # Find cell coordinates \n",
    "                x,y = np.where(mask == cellIDs[cell_id])\n",
    "\n",
    "                # Do not consider cells within 10 pixels from the border\n",
    "                if not (np.any(np.concatenate((x,y)) <10) or np.any(np.concatenate((x,y)) >2038)):\n",
    "\n",
    "                    dapi_int = np.zeros(len(frames))\n",
    "                    for z in range(frames.size):\n",
    "                        Is = dapiStack[z,:,:].squeeze()\n",
    "                        dapiIntensity[cellIDs[cell_id],z] = sum(Is[mask==cellIDs[cell_id]])\n",
    "\n",
    "            np.save(outputFileName, dapiIntensity)\n",
    "        else:\n",
    "            print(f'fov {fov:>3d} already processed. Skipping.', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how things look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(1,3,1):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    gs = fig.add_gridspec(5, 2)\n",
    "\n",
    "    ax0 = fig.add_subplot(gs[:4,0])\n",
    "    ax1 = fig.add_subplot(gs[:4,1])\n",
    "\n",
    "    centerZ = []\n",
    "    counter = 0\n",
    "    for fov in range(130):    \n",
    "        dapiFileName = f'{outputDir}dapi_int/sample_0{sample}/dapi_int_{fov:03d}.npy'\n",
    "        dapiData = np.load(dapiFileName)\n",
    "\n",
    "        for c in range(dapiData.shape[0]):\n",
    "            x = np.arange(dapiData.shape[1])\n",
    "            y = dapiData[c,:]\n",
    "            if sum(y) > 0: \n",
    "\n",
    "                color_num = np.mod(counter,10)\n",
    "\n",
    "                ax0.plot(x,y,alpha=0.1,c=f'C{color_num}')\n",
    "                x_max = y.argmax()\n",
    "                ax0.plot(x_max,y[x_max],'o',c=f'C{color_num}')\n",
    "                centerZ.append(x_max)\n",
    "\n",
    "                y_norm = (y - y.min())/(y.max()-y.min())\n",
    "                ax1.plot(x,y_norm,alpha=0.5,c=f'C{color_num}')\n",
    "                counter+=1\n",
    "\n",
    "\n",
    "    ax0.set_ylabel('Total Dapi intensity (A.U.)')\n",
    "    ax0.set_xticklabels([])\n",
    "    ax0.set_xlim([0,34])\n",
    "    \n",
    "    ax1.set_xlabel('frame')\n",
    "    ax1.set_ylabel('Total Dapi intensity (scaled)')\n",
    "    ax1.set_xlim([0,34])\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[4,0])\n",
    "    ax2.hist(centerZ, bins=40, range=(0,40),color='C0',alpha=0.3,density=True)\n",
    "    ax2.set_xlabel('frame')\n",
    "    ax2.set_xlim([0,34])\n",
    "\n",
    "    fig.savefig(f'j{expName}_16_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(1,3,1):\n",
    "    filteredCellsDapi = []\n",
    "    cellTotalDapi = np.zeros((130,50)) # fov,ncells\n",
    "\n",
    "    for fov in range(130):    \n",
    "        dapiData = np.load(f'{outputDir}dapi_int/sample_0{sample}/dapi_int_{fov:03d}.npy')\n",
    "\n",
    "        for cell in range(dapiData.shape[0]):\n",
    "            x = np.arange(dapiData.shape[1])\n",
    "            y = dapiData[cell,:]\n",
    "            if sum(y) > 0: # only keep cells with real intensity\n",
    "\n",
    "                x_max = y.argmax()\n",
    "                if x_max >=5 and x_max <=30: # discard cells whose center is in the borders of the range\n",
    "                    filteredCellsDapi.append(y[x_max])\n",
    "                    cellTotalDapi[fov,cell] = y[x_max]\n",
    "\n",
    "    np.save(f'{outputDir}dapi_int/sample_0{sample}/filteredCellsDapi.npy',np.array(filteredCellsDapi))   \n",
    "    np.save(f'{outputDir}dapi_int/sample_0{sample}/cellTotalDapi.npy',cellTotalDapi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "microscope = ['merfish3', 'merfish8']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "\n",
    "    # read original and corrected dataset\n",
    "    filteredCellsDapi = np.load(f'{outputDir}dapi_int/sample_0{sample}/filteredCellsDapi.npy')\n",
    "\n",
    "    # flatten the arrays\n",
    "    C = np.ravel(filteredCellsDapi)\n",
    "\n",
    "    # remove zeros\n",
    "    C = C[C>0]\n",
    "\n",
    "    # plot histogram\n",
    "    bin_counts, bin_edges = np.histogram(C, bins=100)\n",
    "    bin_centers = np.array([0.5 * (bin_edges[i] + bin_edges[i+1]) for i in range(len(bin_edges)-1)])\n",
    "    ax[sample-1].bar(bin_centers, bin_counts, width=bin_centers[1] - bin_centers[0], color='C0', label=r'Histogram',alpha=0.7)\n",
    "\n",
    "    # Define fit function.\n",
    "    def two_gaussians(x, A, B, mu1, sigma1, mu2, sigma2):\n",
    "        return (A * np.exp(-1.0 * (x - mu1)**2 / (2 * sigma1**2)) + B * np.exp(-1.0 * (x - mu2)**2 / (2 * sigma2**2)))\n",
    "\n",
    "    # Define fit function.\n",
    "    def single_gaussian(x, A, mu, sigma):\n",
    "        return (A * np.exp(-1.0 * (x - mu)**2 / (2 * sigma**2)))\n",
    "\n",
    "    # fit double gaussian\n",
    "    if sample == 1:\n",
    "        p0 = [400, 50, 8.5e6, 5e5, 1.7e7, 1e6]\n",
    "        xmax = 3e7\n",
    "        ymax = 500\n",
    "    elif sample == 2:\n",
    "        p0 = [250, 26, 5.8e8, 5e7, 1.1e9, 1.1e8]\n",
    "        xmax = 2e9\n",
    "        ymax = 300\n",
    "    \n",
    "    popt, pcov = curve_fit(two_gaussians, xdata=bin_centers, ydata=bin_counts, p0=p0)\n",
    "\n",
    "    # plot fitted function\n",
    "    xspace = np.linspace(0, xmax, 1000)\n",
    "    yspace = two_gaussians(xspace, *popt)\n",
    "    ax[sample-1].plot(xspace, yspace, color='r', linewidth=1, label=r'2-gaussian fit')\n",
    "    ax[sample-1].plot(xspace, single_gaussian(xspace, *popt[[0,2,3]]), 'r--', linewidth=1, label=f'Gaussian1\\n($\\\\mu$ = {popt[2]:.0f}, $\\\\sigma$ = {popt[3]:.0f})')\n",
    "    ax[sample-1].plot(xspace, single_gaussian(xspace, *popt[[1,4,5]]), 'r:', linewidth=1, label=r'Gaussian 2')\n",
    "\n",
    "    # plot areas\n",
    "    a = np.sqrt(2)\n",
    "    m = popt[2]\n",
    "    s = popt[3]\n",
    "    ax[sample-1].fill([  m-  s,   m+  s,   m+  s,   m-  s], [0, 0, ymax, ymax], alpha=0.1, color='r')\n",
    "    ax[sample-1].fill([2*m-a*s, 2*m+a*s, 2*m+a*s, 2*m-a*s], [0, 0, ymax, ymax], alpha=0.1, color='r')\n",
    "    ax[sample-1].text(  m,0.9*ymax,'2n',ha='center',fontsize=12)\n",
    "    ax[sample-1].text(2*m,0.9*ymax,'4n',ha='center',fontsize=12)\n",
    "\n",
    "    ax[sample-1].axis([0,xmax,0,ymax])\n",
    "    ax[sample-1].set_xlabel('Total DAPI intensity (Flat-field corrected)')\n",
    "    ax[sample-1].legend(loc='right')\n",
    "    areaFraction = (popt[0]*popt[3])/(popt[0]*popt[3] + popt[1]*popt[5])\n",
    "    ax[sample-1].set_title(f'sample {sample}, {microscope[sample-1]}\\n(2n cells)/(All cells) = {areaFraction:2.2f}')\n",
    "\n",
    "    # fig.tight_layout(pad=3,w_pad=1,h_pad=1)  # otherwise the right y-label is slightly clipped\n",
    "    # plt.show()\n",
    "    fig.savefig(f'j{expName}_17_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, very good. Let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if mosaics forlder exists\n",
    "if not os.path.isdir(f'{outputDir}cell_info/'):\n",
    "    os.mkdir(f'{outputDir}cell_info/')\n",
    "\n",
    "for sample in range(1,3,1): \n",
    "    \n",
    "    if not os.path.isdir(f'{outputDir}cell_info/sample_0{sample}/'):\n",
    "        os.mkdir(f'{outputDir}cell_info/sample_0{sample}/')\n",
    "\n",
    "    cellTotalDapi = np.load(f'{outputDir}dapi_int/sample_0{sample}/cellTotalDapi.npy')   \n",
    "\n",
    "    # For each channel,\n",
    "    for channel in range(2):\n",
    "        peakThresh = [10**3.1,10**3.4]\n",
    "        for hyb in range(8):\n",
    "\n",
    "            cell_fov   = []\n",
    "            cell_ids   = []\n",
    "            cell_area  = []\n",
    "            cell_dapi  = []\n",
    "            tot_spots  = []\n",
    "            tot_above  = []\n",
    "            tot_height = []\n",
    "\n",
    "            # check that the file has not been made before. \n",
    "\n",
    "            fileName = f'{outputDir}cell_info/sample_0{sample}/h{hyb}_c{channel}.csv'\n",
    "            if not os.path.isfile(fileName): \n",
    "                #print(f'{fileName} not found!')\n",
    "                for fov in range(130):\n",
    "                    # reading spot list for specific fov/hyb/channel \n",
    "                    if hyb == 0:\n",
    "                        imName = f'hal_{fov:03d}'\n",
    "                    else:\n",
    "                        imName = f'hal_{fov:03d}_{hyb-1}'\n",
    "\n",
    "                    spotFileName = f'{outputDir}spots/sample_0{sample}/{imName}_{channel}.npy'\n",
    "\n",
    "                    if os.path.isfile(spotFileName):\n",
    "\n",
    "                        spots = np.load(spotFileName)\n",
    "#                         print(f'\\t{spotFileName} found!')\n",
    "\n",
    "                        # get the contours\n",
    "                        contoursFolder = f'{outputDir}cell_contours/sample_0{sample}'\n",
    "                        contourFileList = os.listdir(contoursFolder)\n",
    "                        contourFileList = [c for c in contourFileList if int(c.split('_')[1]) == fov]\n",
    "                        \n",
    "#                         print(f'\\t{len(contourFileList)} cells found')\n",
    "\n",
    "                        for file_num in range(len(contourFileList)):\n",
    "\n",
    "                            cell_id = int(re.split(r'[_.]',contourFileList[file_num])[2])\n",
    "\n",
    "                            # read cell contour\n",
    "                            contourFileName = f'{contoursFolder}/{contourFileList[file_num]}'\n",
    "                            if os.path.isfile(contourFileName):\n",
    "                                #print(f'reading {contourFileName}', end='\\r')\n",
    "\n",
    "                                contour = np.load(contourFileName)\n",
    "\n",
    "                                # get indexes of spots that are inside the cell\n",
    "                                indexes = []\n",
    "                                for spot_idx in range(spots.shape[0]):\n",
    "                                    y = spots[spot_idx,2]\n",
    "                                    x = spots[spot_idx,3]\n",
    "                                    result = cv2.pointPolygonTest(contour, (x,y), False) \n",
    "                                    if result == 1 or result == 0:\n",
    "                                        indexes.append(spot_idx)\n",
    "                                    #print(f'\\t\\t\\tspot {spot_idx:>3d}. x={x:>6.1f}, y={y:>6.1f}', end='\\r')\n",
    "\n",
    "                                # calculate features\n",
    "                                if cellTotalDapi[fov,cell_id] > 0:\n",
    "                                    tot_spots.append(len(indexes))\n",
    "                                    tot_above.append(sum(spots[indexes,0] > peakThresh[channel]))\n",
    "                                    tot_height.append(sum(spots[indexes,0]))\n",
    "                                    cell_ids.append(cell_id)\n",
    "                                    cell_fov.append(fov)   \n",
    "                                    cell_dapi.append(cellTotalDapi[fov,cell_id] )\n",
    "                                    cell_area.append(cv2.contourArea(contour))\n",
    "                            else:\n",
    "                                print(f'{contourFileName } NOT found!')#,end='\\r')\n",
    "                    else:\n",
    "                        print(f'{spotFileName} NOT found!')#,end='\\r')\n",
    "                        \n",
    "\n",
    "                df = pd.DataFrame({'fov':cell_fov, 'cell_id':cell_ids, 'cell_area':cell_area, 'cell_dapi':cell_dapi, 'tot_spots':tot_spots, 'tot_over_thresh':tot_above, 'tot_height':tot_height})\n",
    "                df.to_csv(fileName)\n",
    "                print(f'{fileName} created.')\n",
    "            else:\n",
    "                print(f'{fileName} already exists. Skipping.')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the number of spots vs the DNA content\n",
    "\n",
    "I will look at a single loci first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meanInt = [8848918, 566995304] # mean\n",
    "stdInt  = [ 947998,  61953814] # standard deviation\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "    \n",
    "    m = meanInt[sample-1]\n",
    "    s = stdInt[sample-1]\n",
    "\n",
    "    # get histogram of the data Add histograms of exponential and gaussian data.\n",
    "\n",
    "    genomic_size = ['5kb', '5kb', '10kb', '10kb', '15kb', '15kb', '20kb', '20kb'] \n",
    "\n",
    "    selectedRegionsDF = pd.read_csv('selected_regions.csv')\n",
    "\n",
    "    for channel in range(2):\n",
    "\n",
    "        fig,ax = plt.subplots(nrows=3,ncols=8,figsize=(15,7))\n",
    "\n",
    "        for hyb in range(8):\n",
    "\n",
    "            df = pd.read_csv(f'{outputDir}cell_info/sample_0{sample}/h{hyb}_c{channel}.csv')\n",
    "            \n",
    "            ylabel = ['Total number of spots','Number of spots\\nover threshold','Sum of spot intensities']\n",
    "            yvar = ['tot_spots','tot_over_thresh','tot_height']\n",
    "            xmax = 6\n",
    "            ymax = [[50,10,50000],[80,10,150000]]\n",
    "            X = 2*df.cell_dapi.to_numpy()/m\n",
    "            for j in range(3):\n",
    "                Y = df[yvar[j]].to_numpy()\n",
    "\n",
    "                # calculate average\n",
    "                xx, xs, yy, ys, n = ma.getMovingAverage(X, Y, bin_method='equal_sample_size', points_per_bin=50)\n",
    "\n",
    "                # plot data and average\n",
    "                ax[j,hyb].plot(X,Y,'o',alpha=0.05,ms=3)\n",
    "                ax[j,hyb].plot(xx,yy,'k.-',ms=2,lw=1)\n",
    "\n",
    "                # adding areas of 2n and 4n cells\n",
    "                a = np.sqrt(2)\n",
    "                mm = 2\n",
    "                ss = 2*s/m\n",
    "                ax[j,hyb].fill([  mm-ss,     mm+ss,     mm+ss,     mm-ss  ], [0, 0, ymax[channel][j], ymax[channel][j]], alpha=0.1, color='r')\n",
    "                ax[j,hyb].fill([2*mm-a*ss, 2*mm+a*ss, 2*mm+a*ss, 2*mm-a*ss], [0, 0, ymax[channel][j], ymax[channel][j]], alpha=0.1, color='r')\n",
    "\n",
    "                # calculate average for Y inside the filled area\n",
    "                meanY2 = Y[(X >=     mm-ss) & (X <     mm+ss)].mean()\n",
    "                meanY4 = Y[(X >= 2*mm-a*ss) & (X < 2*mm+a*ss)].mean()\n",
    "                num_format = [\".1f\",\".1f\",\".0f\"]\n",
    "                ax[j,hyb].text(  mm,0.8*ymax[channel][j],f'2n\\n{meanY2:{num_format[j]}}',ha='center', fontsize=8)\n",
    "                ax[j,hyb].text(2*mm,0.8*ymax[channel][j],f'4n\\n{meanY4:{num_format[j]}}',ha='center', fontsize=8)\n",
    "\n",
    "                ax[j,hyb].plot([0, xmax],[  meanY2, meanY2],'k:')\n",
    "                ax[j,hyb].plot([0, xmax],[2*meanY2,2*meanY2],'k:')\n",
    "\n",
    "                # add labels\n",
    "                if hyb == 0:\n",
    "                    ax[j,hyb].set_ylabel(ylabel[j])#, rotation='horizontal',va='center')\n",
    "                if j == 2:\n",
    "                    ax[j,hyb].set_xlabel('Ploidy')\n",
    "\n",
    "                if j == 0:\n",
    "                    ax[j,hyb].set_title(f'{int(selectedRegionsDF.a.iloc[hyb])} probes ({genomic_size[hyb]})',fontsize=10)\n",
    "                ax[j,hyb].axis([0, xmax, 0, ymax[channel][j] ])\n",
    "\n",
    "                if j < 2:\n",
    "                    ax[j,hyb].set_xticklabels([])\n",
    "                    ax[j,hyb].set_xticks([])\n",
    "                else:\n",
    "                    ax[j,hyb].set_xticks([0,2,4])\n",
    "                    ax[j,hyb].set_xticklabels(['0','2','4'])\n",
    "\n",
    "                if hyb > 0:   \n",
    "                    ax[j,hyb].set_yticklabels([])\n",
    "                    ax[j,hyb].set_yticks([])\n",
    "\n",
    "\n",
    "        fig.tight_layout(pad=3,w_pad=0.5,h_pad=0.5)\n",
    "        fig.savefig(f'j{expName}_18_{expDate}_{channel}_{sample}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all the values are worse than Pu's experiment. I need to think about news experiments, before performing more complex analysis. I need to be able to recapitulate the results by Pu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get histogram of the data Add histograms of exponential and gaussian data.\n",
    "MF = [3,8]\n",
    "meanInt = [8848918, 566995304] # mean\n",
    "stdInt  = [ 947998,  61953814] # standard deviation\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "\n",
    "    ch_names = [750,650]\n",
    "    ch_colors = ['C3', 'C4']\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=8,figsize=(15,5))\n",
    "\n",
    "    p2n = np.zeros((2,8))\n",
    "    m2n = np.zeros((2,8))\n",
    "    \n",
    "    m = 2\n",
    "    s = 2*stdInt[sample-1]/meanInt[sample-1]\n",
    "\n",
    "    for channel in range(2):\n",
    "        for hyb in range(8):\n",
    "            df = pd.read_csv(f'{outputDir}cell_info/sample_0{sample}/h{hyb}_c{channel}.csv')\n",
    "\n",
    "            X = 2*df.cell_dapi.to_numpy()/meanInt[sample-1]\n",
    "            Y = df['tot_over_thresh'].to_numpy()\n",
    "            idx = (X >= (m-s))& (X <= (m+s))\n",
    "\n",
    "            edges = np.arange(12)-0.5\n",
    "            h, edges = np.histogram(Y[idx],bins=edges)\n",
    "            centers = edges[1:] - (edges[1]-edges[0])/2\n",
    "\n",
    "            ax[channel,hyb].plot(centers,h/h.sum(), '.-',c=ch_colors[channel])\n",
    "\n",
    "            p2n[channel,hyb] = h[2]/h.sum()\n",
    "            m2n[channel,hyb] = np.mean(Y[idx])\n",
    "\n",
    "            ax[channel,hyb].axis([0,10,0,1])\n",
    "\n",
    "            if hyb == 0:\n",
    "                ax[channel,hyb].set_ylabel(f'{ch_names[channel]} channel\\nProbability',fontsize=11)\n",
    "            else:\n",
    "                ax[channel,hyb].set_yticks([])\n",
    "                ax[channel,hyb].set_yticklabels([])\n",
    "\n",
    "            if channel == 1:\n",
    "                ax[channel,hyb].set_xlabel('Spots per cell')\n",
    "            else:\n",
    "                ax[channel,hyb].set_xticks([])\n",
    "                ax[channel,hyb].set_xticklabels([])\n",
    "                ax[channel,hyb].set_title(f'{int(selectedRegionsDF.a.iloc[hyb])} probes ({genomic_size[hyb]})',fontsize=11)\n",
    "\n",
    "            ax[channel,hyb].text(1,0.8,f'p(n = 2) = {p2n[channel,hyb]:.2f}',fontsize=10)\n",
    "\n",
    "    fig.suptitle(f'MERFISH{MF[sample-1]}')\n",
    "    fig.tight_layout(pad=3,w_pad=0.5,h_pad=0.5)\n",
    "    fig.savefig(f'j{expName}_19_{expDate}_{sample}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to compare, let's overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get histogram of the data Add histograms of exponential and gaussian data.\n",
    "markers = ['.-','.:']\n",
    "meanInt = [8848918, 566995304] # mean\n",
    "stdInt  = [ 947998,  61953814] # standard deviation\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2,ncols=8,figsize=(15,5))\n",
    "\n",
    "p2n = np.zeros((2,2,8))\n",
    "m2n = np.zeros((2,2,8))\n",
    "\n",
    "for sample in range(1,3,1):\n",
    "\n",
    "    ch_names = [750,650]\n",
    "    ch_colors = ['C3', 'C4']\n",
    "    \n",
    "    m = 2\n",
    "    s = 2*stdInt[sample-1]/meanInt[sample-1]\n",
    "\n",
    "    for channel in range(2):\n",
    "        for hyb in range(8):\n",
    "            df = pd.read_csv(f'{outputDir}cell_info/sample_0{sample}/h{hyb}_c{channel}.csv')\n",
    "\n",
    "            X = 2*df.cell_dapi.to_numpy()/meanInt[sample-1]\n",
    "            Y = df['tot_over_thresh'].to_numpy()\n",
    "            idx = (X >= (m-s))& (X <= (m+s))\n",
    "\n",
    "            edges = np.arange(12)-0.5\n",
    "            h, edges = np.histogram(Y[idx],bins=edges)\n",
    "            centers = edges[1:] - (edges[1]-edges[0])/2\n",
    "\n",
    "            ax[channel,hyb].plot(centers,h/h.sum(), markers[sample-1],c=ch_colors[channel])\n",
    "\n",
    "            p2n[sample-1,channel,hyb] = h[2]/h.sum()\n",
    "            m2n[sample-1,channel,hyb] = np.mean(Y[idx])\n",
    "\n",
    "            ax[channel,hyb].axis([0,10,0,1])\n",
    "\n",
    "            if hyb == 0:\n",
    "                ax[channel,hyb].set_ylabel(f'{ch_names[channel]} channel\\nProbability',fontsize=11)\n",
    "            else:\n",
    "                ax[channel,hyb].set_yticks([])\n",
    "                ax[channel,hyb].set_yticklabels([])\n",
    "\n",
    "            if channel == 1:\n",
    "                ax[channel,hyb].set_xlabel('Spots per cell')\n",
    "            else:\n",
    "                ax[channel,hyb].set_xticks([])\n",
    "                ax[channel,hyb].set_xticklabels([])\n",
    "                ax[channel,hyb].set_title(f'{int(selectedRegionsDF.a.iloc[hyb])} probes ({genomic_size[hyb]})',fontsize=10)\n",
    "\n",
    "            ax[channel,hyb].text(1,0.9-0.1*sample,f'p(n = 2) = {p2n[sample-1,channel,hyb]:.2f}',fontsize=9)\n",
    "\n",
    "    fig.tight_layout(pad=3,w_pad=0.5,h_pad=0.5)\n",
    "    fig.savefig(f'j{expName}_20_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from MF3 and MF8 are almost indistinguishable, which is good. It is reproducible across microscopes. \n",
    "\n",
    "But overall,  things look worse than Pu's data. There with 100 probes I got p(n=2)~ 0.6 (and up to 0.8), a very big difference. \n",
    "\n",
    "Some things to observe, though: Where is the peak for each group?\n",
    "\n",
    "|# probes | 65 | 101 | 143 | 193 | 263 | 298 | 357 | 398 | \n",
    "|---------|----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|MF3, 750 |  1 | 1   |  2  |  2  |  3  |  2  | 3   |  2  |\n",
    "|MF3, 650 |  1 | 1   |  2  |  2  |  3  |  2  | 3   |  2  |\n",
    "|MF8, 750 |  2 | 1   |  2  |  2  |  2  |  2  | 3   |  -  |\n",
    "|MF8, 650 |  2 | 2   |  2  |  2  |  2  |  2  | 3   |  -  |\n",
    "\n",
    "Let's make a summary plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['.-','.:']\n",
    "\n",
    "selectedRegionsDF = pd.read_csv('selected_regions.csv')\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
    "\n",
    "for sample in range(2):\n",
    "\n",
    "    ax[0].plot(selectedRegionsDF.a,p2n[sample-1,0,:],markers[sample-1],c='C3', label='750')\n",
    "    ax[0].plot(selectedRegionsDF.b,p2n[sample-1,1,:],markers[sample-1],c='C4', label='647')\n",
    "    ax[0].set_xlabel('Number of probes')\n",
    "    ax[0].set_ylabel('P(n = 2) ')\n",
    "    ax[0].axis([50, 400, 0, 0.5])\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(selectedRegionsDF.a,m2n[sample-1, 0,:],markers[sample-1],c='C3', label='750')\n",
    "    ax[1].plot(selectedRegionsDF.b,m2n[sample-1, 1,:],markers[sample-1],c='C4', label='647')\n",
    "    ax[1].set_xlabel('Number of probes')\n",
    "    ax[1].set_ylabel('Average number of spots\\n in nuclei')\n",
    "    ax[1].axhline(2,ls='--',c='k', alpha=0.5)\n",
    "    ax[1].axis([50, 400, 0, 4])\n",
    "    ax[1].legend()\n",
    "\n",
    "fig.tight_layout(pad=3,w_pad=3,h_pad=1)\n",
    "fig.savefig(f'j{expName}_21_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the current analysis, the optimal number of probes is in the range 150-200. What is the size of the regions?\n",
    "\n",
    "    group 0 - ( 65) - 5 kb\n",
    "    group 1 - (101) - 5 kb\n",
    "    group 2 - (143) - 10 kb\n",
    "    group 3 - (193) - 10 kb\n",
    "    group 4 - (263) - 15 kb\n",
    "    group 5 - (298) - 15 kb\n",
    "    group 6 - (357) - 20 kb\n",
    "    group 7 - (398) - 20 kb\n",
    "    \n",
    "If we compare groups 0/1, and 2/3, we see that the second (1 and 3) have a higher p2n than the previos group. That could make sense if what I am looking at here is that the results get a little better if the density of probes gets higher. However, from the other two sets of groups (5/6 and 7/8) I get pretty much the opposite. Maybe one way of interpretting this is that the two groups give different results from different reasons. Still, it seems that the pattern is not really clear. \n",
    "\n",
    "let's look @ the density of probes per kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['.-','.:']\n",
    "nProbes = np.array([65, 101, 143, 193, 263, 298, 357, 398])\n",
    "regionSize = np.array([5, 5, 10, 10, 15, 15, 20, 20])\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2,nrows=1, figsize=(10,5))\n",
    "ax[0].plot(nProbes, nProbes/regionSize,'o-')\n",
    "ax[0].axis([50,400,0,25])\n",
    "ax[0].set_xlabel('Number of probes')\n",
    "ax[0].set_ylabel('Probe density (probes / kb)')\n",
    "\n",
    "for sample in range(2):\n",
    "\n",
    "    ax[1].plot(nProbes/regionSize,p2n[sample-1,0,:],markers[sample-1],c='C3', label='750')\n",
    "    ax[1].plot(nProbes/regionSize,p2n[sample-1,1,:],markers[sample-1],c='C4', label='647')\n",
    "    ax[1].set_xlabel('Probe density (probes / kb)')\n",
    "    ax[1].set_ylabel('P(n = 2) ')\n",
    "#     ax[1].axis([50, 400, 0, 0.5])\n",
    "    ax[1].legend()\n",
    "    \n",
    "fig.savefig(f'j{expName}_22_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not see a clear pattern. Which other parameters can explain the behavior?\n",
    "   - Tm?\n",
    "   - %GC?\n",
    "    \n",
    "I could calculate that for Pu's data and correlate with p2n for different regions.  \n",
    "\n",
    "# Going back to CNV003\n",
    "\n",
    "The first thing is to get the sequences for each of the regions used in the experiment analyzed in the CNV003 experiment. The information about the colors is in  'v:/20200707-IMR90_SI16-5kb/Analysis/Color_Usage.csv':\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question here is which are the sequences of these \"u\". I sent an email to Pu:\n",
    "    \n",
    "    Hi Pu, \n",
    "\n",
    "    I hope you are doing well. \n",
    "\n",
    "    I am revisiting the analysis I did of one of your experiments, particularly, the one in the /20200707-IMR90_SI16-5kb/ folder. I would like to know the sequences for the encoding probes used for that experiment. \n",
    "\n",
    "    Inside the folder above there is another folder named \"analysis\" and a file called Color_Usage.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorUsageDF = pd.read_csv(f'v:/20200707-IMR90_SI16-5kb/Analysis/Color_Usage.csv')\n",
    "colorUsageDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    I am looking here for the sequences of u0, u1,  .. etc.\n",
    "\n",
    "    I found a file called \"adaptor_sequences.csv\" inside the  \\\\10.245.74.212\\Chromatin_NAS_2\\Libraries\\SI16\\color_usage_info\\ folder, with the following info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptorSequencesDF = pd.read_csv(f'w:/Libraries/SI16/color_usage_info/adaptor_sequences.csv')\n",
    "adaptorSequencesDF.head(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Does this mean that\n",
    "    u0 -> NDB_784\n",
    "    u1 -> NDB_755 \n",
    "    u2 -> NDB_759\n",
    "\n",
    "    etc?\n",
    "\n",
    "    Thanks, \n",
    "\n",
    "    Leonardo\n",
    "    \n",
    "His response:\n",
    "\n",
    "\n",
    "    Hi Leonardo,\n",
    "\n",
    "    Your interpretation is correct. color_usage directs the encoding bits, then adaptor_sequence converts bits into readout sequences, the final readout sequences could be found here:\n",
    "    \\\\10.245.74.212\\Chromatin_NAS_2\\Libraries\\Readouts\\designed_readouts_xxx.fasta\n",
    "\n",
    "\n",
    "    Best,\n",
    "    Pu\n",
    "\n",
    "Ok, let's get the names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDB = np.zeros((32,3)).astype(int)\n",
    "readouts = [750, 647, 561]\n",
    "\n",
    "for hyb in range(32):\n",
    "    for ch in range(3):\n",
    "        NDB[hyb,ch] = int(adaptorSequencesDF[f'{readouts[ch]}_readout'].iloc[hyb].split('_')[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the sequences for the encoding probes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = list(SeqIO.parse(\"w:/Libraries/DNA-MERFISH_Seon/SI16.fasta\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGCContent(seq):\n",
    "    return (seq.count('C') + seq.count('G'))/len(seq)\n",
    "\n",
    "def getProbeSeparation(probePositionList):\n",
    "    ps = np.zeros(len(probePositionList)-1)\n",
    "    for i in range(ps.size):\n",
    "        ps[i] = probePositionList[i+1] - probePositionList[i]\n",
    "    return ps\n",
    "    \n",
    "numProbes = np.zeros((32,3)).astype(int)\n",
    "meanGcContent = np.zeros((32,3))\n",
    "meanProbeSeparation = np.zeros((32,3))\n",
    "stdProbeSeparation = np.zeros((32,3))\n",
    "\n",
    "for hyb in range(32):\n",
    "    for ch in range(3):\n",
    "        encodingProbeSeqs = []\n",
    "        probePositions = []\n",
    "        for ii in range(len(fasta)):\n",
    "            if 'chr21' in fasta[ii].id and 'res5000' in fasta[ii].id:\n",
    "                if NDB[hyb,ch] == int(fasta[ii].id.split(':')[-1].split('_')[1]):\n",
    "                    encodingProbeSeqs.append(f'{fasta[ii].seq}'[40:80])\n",
    "                    probePositions.append(int(fasta[ii].id.split('_')[6]))\n",
    "\n",
    "        gcContent = [getGCContent(x) for x in encodingProbeSeqs]\n",
    "        probeSeparation = getProbeSeparation(probePositions)\n",
    "        \n",
    "        # get average values\n",
    "        numProbes[hyb,ch] = len(gcContent)\n",
    "        meanProbeSeparation[hyb,ch] = np.mean(probeSeparation)\n",
    "        stdProbeSeparation[hyb,ch] = np.std(probeSeparation)\n",
    "        meanGcContent[hyb,ch] = np.mean(gcContent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now compare p2n for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 28964755/1e8   # mean\n",
    "s =  2693210/1e8   # standard deviation\n",
    "\n",
    "titles = [750,650,560]\n",
    "\n",
    "p2n = np.zeros((32,3))\n",
    "\n",
    "for ch in range(3):\n",
    "    for hyb in range(1,32,1):\n",
    "        df = pd.read_csv(f'g:/analysis/CNVs/CNV003/cell_info/H{hyb}R{hyb}_{ch}.csv')\n",
    "  \n",
    "        X = df.cell_dapi.to_numpy()/1e8\n",
    "        Y = df['tot_over_thresh'].to_numpy()\n",
    "        idx = (X >= (m-s))& (X <= (m+s))\n",
    "        \n",
    "        edges = np.arange(12)-0.5\n",
    "        h, edges = np.histogram(Y[idx],bins=edges)\n",
    "        \n",
    "        p2n[hyb,ch] = (h[2]/h.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare p2n with other of the multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,nrows=3,figsize=(8,8))\n",
    "\n",
    "xlabels = ['Number of probes', 'Mean probe separation', 'mean GC content']\n",
    "channelNames = ['750','647','561']\n",
    "xmin = [ 40, 40, 0.2]\n",
    "xmax = [120,120, 0.6]\n",
    "\n",
    "for xvar in range(3):\n",
    "    if xvar == 0:\n",
    "        x = numProbes[:,ch]\n",
    "    elif xvar == 1:\n",
    "        x = meanProbeSeparation[:,ch]\n",
    "    elif xvar == 2:\n",
    "        x = meanGcContent[:,ch]\n",
    "\n",
    "    for ch in range(3):\n",
    "        ax[xvar,ch].plot(x,p2n[:,ch],'o')\n",
    "        ax[xvar,ch].set_xlabel(xlabels[xvar])\n",
    "        ax[xvar,ch].set_ylabel('p2n')\n",
    "        if xvar == 0:\n",
    "            ax[xvar,ch].set_title(channelNames[ch])\n",
    "        ax[xvar,ch].axis([xmin[xvar],xmax[xvar],0,1])\n",
    "\n",
    "fig.tight_layout(pad=3,w_pad=1,h_pad=1)\n",
    "fig.savefig(f'j{expName}_23_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no discernable pattern. So, there should be something else I can do to try getting higher labeling efficiencies. \n",
    "\n",
    "But there is something weird. going on. Are there correlations between the different channels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(9,3))\n",
    "\n",
    "xlabels = ['Number of probes', 'Mean probe separation', 'mean GC content']\n",
    "channelNames = ['750','647','561']\n",
    "\n",
    "\n",
    "for ch in range(3):\n",
    "    ax[ch].plot([0,1],[0,1],'r-',alpha=1,lw=0.5)\n",
    "    ax[ch].plot(p2n[:,ch],p2n[:,np.mod(ch+1,3)],'+')\n",
    "    ax[ch].set_xlabel(f'p2n, {channelNames[ch]}')\n",
    "    ax[ch].set_ylabel(f'p2n, {channelNames[np.mod(ch+1,3)]}')\n",
    "    ax[ch].axis([0,1,0,1])\n",
    "\n",
    "fig.tight_layout(pad=1,w_pad=0,h_pad=0)\n",
    "fig.savefig(f'j{expName}_24_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a weak correlation between the p2n in 750 and 650 channels, suggesting that there is a variation in the specific hybs. Is there a relationship between the number of the hyb and the p2n? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=1,nrows=1,figsize=(9,3))\n",
    "\n",
    "xlabels = ['Number of probes', 'Mean probe separation', 'mean GC content']\n",
    "channelNames = ['750','647','561']\n",
    "\n",
    "x = np.arange(p2n.shape[0])\n",
    "for ch in range(3):\n",
    "    ax.plot(x,p2n[:,ch],'o-',label=f'{channelNames[ch]}')\n",
    "    ax.set_xlabel('Hyb number')\n",
    "    ax.set_ylabel('p2n')\n",
    "    ax.axis([0,max(x),0,1])\n",
    "ax.legend('on')\n",
    "fig.tight_layout(pad=1,w_pad=0,h_pad=0)\n",
    "fig.savefig(f'j{expName}_25_{expDate}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious change in p2n with hybridization round.\n",
    "\n",
    "But still there is the question of why I am not getting the same results as Pu. That suggests that still I need to work in reproducibility. What are possible parameters?\n",
    "\n",
    "    - Nicking solution concentration/time:\n",
    "        This seem like an interesting posibility. I need to check where do they get the concentration of Hcl or the time. Maybe someone Has measured the relationship between Hcl and the nicking. \n",
    "    - Hybridization solution formamide concentration\n",
    "        This in case I have not prepared the solutions properly. But that sounds weird. \n",
    "    - Washing times\n",
    "        The same as before. Unlikely.\n",
    "    - Amplification level.\n",
    "        I am using the same level of amplification as Pu. But maybe another experiment I can get some additional amplification?\n",
    "    - Something cell-type specific. \n",
    "\n",
    "\n",
    "I think that another simple test is to repeat his experiment, use the same chromosomes areas as in the CNV3 experiment, and see if I can recapitulate his results. \n",
    "\n",
    "So, I have several things to check\n",
    "\n",
    "\t1. What is the effect of Hcl in the nicking of DNA? How does that affect the labelling efficiency?\n",
    "\t2. What is the design that Steven Wang used? What about Alistair?\n",
    "\t3. What is the resolution for the current scDNAseq methods?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syschk.backupNotebook(destination='C:/Software/python-notebooks-backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_env_210121",
   "language": "python",
   "name": "scanpy_env_210121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
